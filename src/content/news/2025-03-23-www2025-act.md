---
title: 北大WWW 2025大模型论文：低成本免微调！缓解大语言模型"知"与"言"鸿沟所导致的幻觉
date: 2025-03-23
excerpt: 北京大学王亚沙教授、马连韬研究助理教授团队在互联网国际学术会议WWW 2025（计算机学会CCF-A类推荐）上发表了题为"Adaptive Activation Steering: A Tuning-Free LLM Truthfulness Improvement Method for Diverse Hallucinations Categories"的研究论文。
featured: true
tags: ['WWW2025', '大语言模型', '幻觉问题', '机器学习', '深度学习']
category: research
image: /images/news/www2025-llm-hallucination-header.jpg
---

北京大学王亚沙教授、马连韬研究助理教授团队在互联网国际学术会议WWW 2025（计算机学会CCF-A类推荐）上发表了题为"Adaptive Activation Steering: A Tuning-Free LLM Truthfulness Improvement Method for Diverse Hallucinations Categories"的研究论文。该研究针对大语言模型（LLM）生成内容中普遍存在的"幻觉"问题，引入了一种无需模型微调的自适应激活转向方法（Adaptive Activation Steering, ACT）提升回答的真实性。

该研究由北京大学、爱丁堡大学、北京航空航天大学等机构联合完成，作者包括：王天龙、焦贤锋、朱英豪、陈忠智、何逸凡、初旭、高峻逸、王亚沙、马连韬。

## 研究背景： 大语言模型的"知"与"言"鸿沟

尽管LLM在文本生成任务中展现出强大能力，但其生成的虚假陈述（即"幻觉"）始终是落地应用的核心挑战。尤其在医疗、法律、教育等高风险场景中，错误信息可能引发严重后果。

LLM 的幻觉可能源于各种因素，如盲目跟从指令、数据噪声、缺乏知识等，但在此之前，一个重要的研究问题是：如果在本应拥有正确知识的前提下，LLM 是否能够始终生成真实的回答？

现有研究表明，LLM在一些情况下内部虽具备对正确知识的理解，却无法稳定输出真实答案，形成"知而不言"的认知鸿沟。研究团队主要关注这类此类现象所导致的幻觉问题。

传统方法如监督微调（SFT）、基于人类反馈的强化学习（RLHF）虽能缓解问题，但存在计算资源消耗大、可能损害模型通用能力等局限性。

## 改进方法： 自适应激活转向（ACT）

计算机视觉（CV）和自然语言处理（NLP）领域对模型内部结构观察和实践启发了研究团队提出一种无需微调的自适应激活转向方法（ACT）来弥合LLM "知而不言"的鸿沟。

在CV领域，现有研究表明图像生成模型的隐空间中存在概念的表征，从而能够实现对生成内容的反事实编辑。类似地，NLP领域的研究发现，人类可解释性概念在LLM中是线性编码的。通过提取特定概念（如公平、无害等）的表征，可以在推理阶段将LLM的输出导向特定概念。

基于这些工作，团队将"真实性"视作LLM中一种特殊的概念，将其线性编码为"转向向量"，并通过在推理阶段应用"转向向量"进行自适应激活转向，将LLM的思维过程导向"真实"方向，弥合LLM "知而不言"的鸿沟。

### 核心创新点

**1. 多个"转向向量"对不同类型幻觉进行定制干预**：团队首次发现不同类别幻觉（如逻辑谬误、常识混淆等）对应的"转向向量"在激活空间中呈现不同的聚类模式，据此设计多个"转向向量"针对性干预不同类型幻觉。

**2. 自适应干预强度控制**：基于LLM激活本身的 "真实性" 含量，动态调整干预强度——对高幻觉风险的激活施加更强干预，避免"一刀切"式调整，从而实现更具针对性的干预效果。

ACT作为一种零微调方法，仅需少量样本即可求解"转向向量"，推理时仅增加常数级时间开销便可显著提升LLM真实性，并适用于不同规模的模型。

## 实验结果： 多角度验证

研究团队在3类基准测试（TruthfulQA，Natural Questions，MMLU），6大主流开源模型（LLaMA，LLaMA2，Alpaca，Vicuna，LLaMA2-Chat，LLaMA3），4种模型规模（7B，13B，33B，65B）评估了ACT方法的有效性。

### 权威评测基准TruthfulQA测试

在OpenAI 发布的评估LLM可信度的关键基准TruthfulQA 上，ACT在所有38种类型的幻觉中均取得性能提升，并在自动化评估和人类专家评估中均显著优于基线方法。

- 在小样本场景下，本文方法在基座模型 LLaMA-7B 的基础上，将 True * Info 指标提升了 70.0%
- 本节还验证了本文方法与少样本提示（FSP）的可叠加性。本文方法结合 FSP 相较于仅使用 FSP 提高了 20.0%
- 在全数据二折交叉验证下，本文方法在 LLaMA-7B基座模型上针对 True*Info 指标提升了 83.1%，并且比表现最佳的 Baseline 方法高出 33.9%

### 跨模型泛化能力

ACT 显著提升多个主流开源模型的真实性，包括LLaMA (↑ 142%)，LLaMA2 (↑ 24%)，Alpaca (↑ 36%)，Vicuna (↑ 28%)，LLaMA2-Chat (↑19%)和LLaMA3(↑ 34%)。

### 跨任务泛化能力

在自然问题回答（Natural Questions）与多任务语言理解（MMLU）等真实场景任务中，ACT同样显著优于基线模型。

### 计算效率

理论分析证明ACT 在推理时只额外引入常数级时间复杂度，实际实验测试ACT推理时仅增加2%计算开销，表明ACT应用于真实世界业务场景的可行性。

### 不同模型规模适用性

ACT 在不同规模模型上的有效性均显著提升了模型的可信度，证明了方法的普适性和可扩展性。

## 总结与展望

该研究提出了自适应激活转向（ACT），促进弥合大语言模型"知"与"言"的鸿沟，为提升LLM生成内容的真实性提供了一种可选的轻量化解决方案。未来团队将进一步探索ACT在多轮对话、多模态生成等复杂场景中的应用潜力。

此外，基于ACT的核心原理在于识别并调整LLM激活空间中的概念分布，该方法还可拓展应用于风格迁移、AI数字人等领域。团队将持续推进技术创新，解锁ACT在更广泛场景中的应用可能性。

## 代码开源

- ArXiv论文链接：https://arxiv.org/abs/2406.00034
- GitHub代码仓库：https://github.com/tianlwang/ACT

---

*医维矩阵实验室*
*2025年3月23日*